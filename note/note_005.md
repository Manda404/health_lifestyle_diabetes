# ğŸ“ **COURS : Lâ€™Ã‰VALUATION DES MODÃˆLES DE CLASSIFICATION EN DATA SCIENCE**

## *La vÃ©ritÃ© sur les mÃ©triques dans le monde rÃ©el (datasets dÃ©sÃ©quilibrÃ©s)*

---

# ğŸ§© **INTRODUCTION : Pourquoi lâ€™Ã©valuation nâ€™est pas une Ã©tape facultative**

CrÃ©er un modÃ¨le qui sâ€™entraÃ®ne nâ€™est **pas** suffisant.
CrÃ©er un modÃ¨le qui gÃ©nÃ©ralise **nâ€™est toujours pas** suffisant.
CrÃ©er un modÃ¨le qui est performant **nâ€™est encore pas suffisant**.

La seule vraie question en entreprise est :

> **Ton modÃ¨le prend-il de BONNES dÃ©cisions dans le contexte mÃ©tier ?**

Et cela dÃ©pend **entiÃ¨rement** de lâ€™Ã©valuation.

L'Ã©valuation te permet de :

* comprendre **oÃ¹ le modÃ¨le rÃ©ussit**
* comprendre **oÃ¹ il Ã©choue**
* savoir s'il est **robuste**
* savoir s'il est **digne d'Ãªtre mis en production**
* savoir **le risque** associÃ© Ã  ses erreurs

Ce cours te donne **la boÃ®te Ã  outils complÃ¨te**.

---

# ğŸ§¨ **PARTIE 1 â€” Les erreurs capitales que font les dÃ©butants**

### âŒ 1. Se fier Ã  lâ€™Accuracy

Dans un dataset dÃ©sÃ©quilibrÃ© :

* si 95% des gens ne sont pas malades,
* un modÃ¨le qui prÃ©dit toujours "pas malade"
* obtient **95% accuracy**

â¡ï¸ et pourtant il est **inutilisable**.

### âŒ 2. Regarder uniquement F1 ou Precision

Une mÃ©trique seule **ne raconte pas lâ€™histoire complÃ¨te**.

### âŒ 3. Ne pas analyser la calibration

Ton modÃ¨le peut Ãªtre "bon" mais **inutilisable pour prendre des dÃ©cisions** si ses probabilitÃ©s ne sont pas fiables.

### âŒ 4. Ne pas analyser les FP et FN

Tu dois comprendre **la nature des erreurs**, pas seulement leur nombre.

---

# ğŸ“Š **PARTIE 2 â€” Les mÃ©triques indispensables en classification dÃ©sÃ©quilibrÃ©e**

On les regroupe en **4 familles**, chacune indispensable.

---

# ğŸ…°ï¸ FAMILLE 1 â€” MÃ‰TRIQUES BASIQUES (mais obligatoires)

Objectif : vÃ©rifier la performance Ã©lÃ©mentaire du modÃ¨le.

## 1ï¸âƒ£ Accuracy (âš  Ã  utiliser avec prudence)

### Pourquoi ?

Donne une premiÃ¨re idÃ©e de la performance **globale**.

### Pourquoi elle est dangereuse ?

Dans les datasets dÃ©sÃ©quilibrÃ©s, elle devient **illusoire**.

ğŸ‘‰ Toujours regarder **balanced accuracy** plutÃ´t que accuracy.

---

## 2ï¸âƒ£ Precision

[
Precision = \frac{TP}{TP + FP}
]

### Pourquoi elle est cruciale ?

* mesure combien de prÃ©dictions positives sont correctes
* utile si les faux positifs coÃ»tent cher (fausse alerte mÃ©dicale, fraudeâ€¦)

### Quand l'utiliser ?

* si ton problÃ¨me nÃ©cessite de **rÃ©duire les fausses alertes**

---

## 3ï¸âƒ£ Recall (Sensitivity, TPR)

[
Recall = \frac{TP}{TP + FN}
]

### Pourquoi elle est cruciale ?

* mesure la capacitÃ© du modÃ¨le Ã  **attraper la classe positive**
* indispensable en santÃ©, assurance, fraude

### Quand l'utiliser ?

* si rater un positif est **grave**
  â†’ diabÃ¨te, cancer, fraude, dÃ©faut bancaireâ€¦

---

## 4ï¸âƒ£ F1-score

[
F1 = 2 * \frac{Precision * Recall}{Precision + Recall}
]

### Pourquoi indispensable ?

* Ã©quilibre entre precision & recall
* bon choix lorsque tu veux optimiser **les deux** en mÃªme temps

### Attention :

* F1-score masque la rÃ©alitÃ© si les classes sont trÃ¨s dÃ©sÃ©quilibrÃ©es.

---

# ğŸ…±ï¸ FAMILLE 2 â€” MÃ‰TRIQUES ADAPTÃ‰ES AUX DATASETS DÃ‰SÃ‰QUILIBRÃ‰S

## 5ï¸âƒ£ Balanced Accuracy

[
BA = \frac{Recall + Specificity}{2}
]

### Pourquoi indispensable ?

* corrige le biais de lâ€™accuracy
* chaque classe compte **comme si elle Ã©tait Ã©quilibrÃ©e**

### Quand lâ€™utiliser ?

TOUJOURS lorsque ton dataset est dÃ©sÃ©quilibrÃ©.

---

## 6ï¸âƒ£ Specificity (TNR)

[
Specificity = \frac{TN}{TN + FP}
]

### Pourquoi utile ?

* mesure la capacitÃ© Ã  reconnaÃ®tre les nÃ©gatifs
* important pour Ã©viter les faux positifs

---

## 7ï¸âƒ£ FPR (False Positive Rate)

[
FPR = \frac{FP}{FP + TN}
]

### Pourquoi indispensable ?

* mesure la proportion de nÃ©gatifs mal classÃ©s
* critique pour comprendre comment se comporte ton modÃ¨le lorsque la classe majoritaire prÃ©domine

---

## 8ï¸âƒ£ FNR (False Negative Rate)

[
FNR = \frac{FN}{FN + TP}
]

### Pourquoi indispensable ?

* un modÃ¨le peut Ãªtre "excellent" mais rater Ã©normÃ©ment la classe minoritaire
* FNR est critique â†’ les FN sont **les erreurs les plus dangereuses**

### Exemple :

Rater un diabÃ©tique â†’ consÃ©quence grave.

---

# ğŸ…¾ï¸ FAMILLE 3 â€” MÃ‰TRIQUES AVANCÃ‰ES POUR DATASETS DÃ‰SÃ‰QUILIBRÃ‰S

## 9ï¸âƒ£ MCC â€” **Matthews Correlation Coefficient**

[
MCC = \frac{TP Â· TN - FP Â· FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}
]

### Pourquoi câ€™est la mÃ©trique prÃ©fÃ©rÃ©e des experts ?

* tient compte **de toutes les cellules** de la matrice de confusion
* robuste aux distributions dÃ©sÃ©quilibrÃ©es
* impossible Ã  manipuler
* score global **stable**, contrairement Ã  F1, Accuracyâ€¦

### InterprÃ©tation :

* 1 â†’ parfait
* 0 â†’ hasard
* < 0 â†’ pire que hasard

â¡ï¸ MCC est **la mÃ©trique la plus fiable en classification binaire**.

---

## ğŸ”Ÿ Cohen's Kappa

[
\kappa = \frac{P_o - P_e}{1 - P_e}
]

### Pourquoi pertinent ?

* mesure lâ€™accord modÃ¨le â†” rÃ©alitÃ© **en tenant compte du hasard**
* excellent pour Ã©valuer un modÃ¨le dans des milieux oÃ¹ les classes minoritaires importent

### Pourquoi moins utilisÃ© que MCC ?

* un peu instable quand la prÃ©valence change
* MCC donne souvent une mesure plus fidÃ¨le

â¡ï¸ Mais Kappa reste **essentiel pour un audit complet**.

---

# ğŸ…¿ï¸ FAMILLE 4 â€” MÃ‰TRIQUES BASÃ‰ES SUR LES PROBABILITÃ‰S

## 1ï¸âƒ£1ï¸âƒ£ AUC-ROC

### Pourquoi utile ?

* mesure la capacitÃ© du modÃ¨le Ã  sÃ©parer les classes **Ã  tous les seuils**
* trÃ¨s utilisÃ© en compÃ©tition & recherche

### Limite :

* peut Ãªtre **trompeuse** en cas de dataset trÃ¨s dÃ©sÃ©quilibrÃ©

---

## 1ï¸âƒ£2ï¸âƒ£ AUC Precision-Recall

### Pourquoi cruciale en dataset dÃ©sÃ©quilibrÃ© ?

* la PR-AUC se concentre uniquement sur la classe positive
* beaucoup plus informative que ROC-AUC dans ton contexte

â¡ï¸ **Une des mÃ©triques les plus importantes dans ton cas.**

---

## 1ï¸âƒ£3ï¸âƒ£ Brier Score

[
BS = \frac{1}{N}\sum (y_i - p_i)^2
]

### Pourquoi indispensable ?

* mesure la **calibration** des probabilitÃ©s
* essentiel en assurance, santÃ©, scoring de risque

### Exemple :

Un modÃ¨le peut Ãªtre bon en classification, mais **donner des probabilitÃ©s inutilisables**.

---

## 1ï¸âƒ£4ï¸âƒ£ Calibration Curve (et ECE si avancÃ©)

### Pourquoi indispensable ?

* vÃ©rifie si le modÃ¨le est **fiable quand il donne une probabilitÃ©**
* si il prÃ©dit 0.7 â†’ doit Ãªtre vrai 70% du temps

---

# ğŸ¯ **PARTIE 3 â€” Quelle mÃ©trique utiliser et quand ? (TABLEAU PRATIQUE)**

| ScÃ©nario                          | MÃ©trique(s) clÃ©                |
| --------------------------------- | ------------------------------ |
| Dataset dÃ©sÃ©quilibrÃ©              | MCC, Balanced Accuracy, PR-AUC |
| Rater un positif est grave        | Recall, FNR                    |
| Faux positifs coÃ»teux             | Precision, FPR                 |
| Analyse globale robuste           | MCC, ROC-AUC                   |
| DÃ©cisions basÃ©es sur probabilitÃ©s | Brier Score, Calibration curve |
| Comparaison entre modÃ¨les         | ROC-AUC + PR-AUC + MCC         |

---

# ğŸ§  **PARTIE 4 â€” La stratÃ©gie dâ€™Ã©valuation dâ€™un Data Scientist confirmÃ©**

Un expert ne regarde pas "une mÃ©trique".
Il regarde un **ensemble de mÃ©triques**, et surtout **leur cohÃ©rence entre elles**.

### âœ” 1. Toujours regarder MCC

MCC est le premier indicateur de robustesse.

### âœ” 2. Ensuite regarder Recall, Precision, FNR, FPR

â†’ comprendre la nature des erreurs.

### âœ” 3. Ensuite PR-AUC

â†’ qualitÃ© de dÃ©tection de la classe minoritaire.

### âœ” 4. Ensuite Brier Score + calibration

â†’ qualitÃ© des probabilitÃ©s.

### âœ” 5. Ensuite les plots de probas

â†’ comprendre comment le modÃ¨le pense.

**Câ€™est Ã§a, lâ€™Ã©valuation professionnelle.**

---

# ğŸ **CONCLUSION DU COURS : Les mÃ©triques que tu dois ABSOLUMENT maÃ®triser**

### ğŸ”¥ Les indispensables (Ã  regarder systÃ©matiquement)

* **MCC**
* **PR-AUC**
* **Recall**
* **Precision**
* **F1**
* **Balanced Accuracy**
* **FNR / FPR**
* **Brier Score**
* **Calibration Curve**

### ğŸ”¥ Les secondaires mais trÃ¨s utiles

* **ROC-AUC**
* **Specificity**
* **Kappa**

### ğŸ”¥ Les piÃ¨ges

* Accuracy seule
* F1 seul
* ROC-AUC seul
* regarder aucune mÃ©trique de calibration

---